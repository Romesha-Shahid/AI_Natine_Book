{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[],"bookSidebar":[{"type":"link","href":"/humanoid_robotic/docs/intro","label":"Introduction","docId":"intro","unlisted":false},{"type":"link","href":"/humanoid_robotic/docs/physical-ai","label":"Foundations of Physical AI","docId":"physical-ai","unlisted":false},{"type":"link","href":"/humanoid_robotic/docs/ros2","label":"ROS 2: The Robotic Nervous System","docId":"ros2","unlisted":false},{"type":"link","href":"/humanoid_robotic/docs/gazebo-unity","label":"Gazebo + Unity: Digital Twin Pipelines","docId":"gazebo-unity","unlisted":false},{"type":"link","href":"/humanoid_robotic/docs/isaac","label":"NVIDIA Isaac: The AI-Robot Brain","docId":"isaac","unlisted":false},{"type":"link","href":"/humanoid_robotic/docs/vla","label":"Vision-Language-Action (VLA) Pipelines","docId":"vla","unlisted":false},{"type":"link","href":"/humanoid_robotic/docs/humanoid-systems","label":"Humanoid Robotics: Kinematics, Balance, Manipulation","docId":"humanoid-systems","unlisted":false},{"type":"link","href":"/humanoid_robotic/docs/conversational-ai","label":"Conversational Robotics: LLMs, Whisper, Multimodal","docId":"conversational-ai","unlisted":false},{"type":"link","href":"/humanoid_robotic/docs/hardware-architecture","label":"Hardware Architecture: RTX Workstations + Jetson Kits","docId":"hardware-architecture","unlisted":false},{"type":"link","href":"/humanoid_robotic/docs/capstone","label":"Capstone: Autonomous Humanoid Pipeline","docId":"capstone","unlisted":false},{"type":"link","href":"/humanoid_robotic/docs/glossary","label":"Glossary","docId":"glossary","unlisted":false},{"type":"link","href":"/humanoid_robotic/docs/references","label":"References","docId":"references","unlisted":false}]},"docs":{"capstone":{"id":"capstone","title":"Capstone: Autonomous Humanoid Pipeline","description":"Throughout this book, we have explored the foundational components required to build intelligent humanoid robots. We began with the robotic nervous system provided by ROS 2, moved through the digital twin concept with Gazebo and Unity, delved into the AI-robot brain powered by NVIDIA Isaac for perception and simulation, and finally examined Vision-Language-Action (VLA) pipelines for natural interaction. This capstone chapter synthesizes these concepts, presenting a high-level overview of a complete autonomous humanoid pipeline, demonstrating how these diverse technologies coalesce to create truly intelligent physical AI.","sidebar":"bookSidebar"},"conversational-ai":{"id":"conversational-ai","title":"Conversational Robotics: LLMs, Whisper, Multimodal","description":"The ability to engage in natural language conversations is a hallmark of human intelligence. For humanoid robots, integrating advanced conversational AI—powered by Large Language Models (LLMs), speech recognition (like Whisper), and multimodal understanding—is crucial for intuitive human-robot interaction (HRI). This chapter explores how these technologies are converging to create robots that can not only understand commands but also engage in meaningful dialogues, interpret context, and respond appropriately in the physical world.","sidebar":"bookSidebar"},"gazebo-unity":{"id":"gazebo-unity","title":"Gazebo + Unity: Digital Twin Pipelines","description":"In the realm of Physical AI and humanoid robotics, developing and testing intelligent systems directly on hardware is often prohibitive due to cost, time constraints, and safety concerns. This is where digital twins and high-fidelity simulation environments become indispensable. A digital twin is a virtual replica of a physical system, continuously updated with data from its real-world counterpart, allowing for testing, analysis, and optimization in a safe, controlled digital space. This chapter explores how Gazebo and Unity are leveraged to create these digital twins, forming powerful simulation pipelines for humanoid robots.","sidebar":"bookSidebar"},"glossary":{"id":"glossary","title":"Glossary","description":"This section provides definitions for key technical terms used throughout the book.","sidebar":"bookSidebar"},"hardware-architecture":{"id":"hardware-architecture","title":"Hardware Architecture: RTX Workstations + Jetson Kits","description":"The computational demands of Physical AI, especially in humanoid robotics, require a diverse and powerful hardware ecosystem. From training complex AI models and running high-fidelity simulations to deploying real-time perception and control algorithms on the robot itself, the choice of hardware significantly impacts performance, latency, and overall system capabilities. This chapter explores the roles of NVIDIA RTX workstations and Jetson edge AI kits, outlining their architectural features and the trade-offs involved in their integration.","sidebar":"bookSidebar"},"humanoid-systems":{"id":"humanoid-systems","title":"Humanoid Robotics: Kinematics, Balance, Manipulation","description":"Humanoid robots present a unique set of challenges and opportunities in the field of Physical AI. Their human-like form, while offering the potential to operate in environments designed for humans, demands sophisticated control over their many degrees of freedom, intricate balance mechanisms, and dexterous manipulation capabilities. This chapter delves into the fundamental principles governing these aspects of humanoid robotics.","sidebar":"bookSidebar"},"intro":{"id":"intro","title":"Introduction","description":"Welcome to \"Physical AI & Humanoid Robotics: Bridging Digital AI to Real-World Robots\"! This book is designed for graduate and advanced undergraduate students, AI researchers, and educators interested in the fascinating intersection of embodied AI and humanoid robotics. Our journey will bridge the conceptual gap between powerful digital AI models and their physical manifestation in real-world robotic systems.","sidebar":"bookSidebar"},"isaac":{"id":"isaac","title":"NVIDIA Isaac: The AI-Robot Brain","description":"NVIDIA Isaac is a comprehensive platform for the development, simulation, and deployment of AI-powered robotics. It bridges the gap between digital AI and physical robots, offering specialized tools for high-fidelity simulation (Isaac Sim), perception (Isaac ROS), and robot control. For humanoid robotics, Isaac acts as a powerful \"AI-Robot Brain,\" enabling capabilities like advanced perception, simultaneous localization and mapping (SLAM), and robust sim-to-real transfer.","sidebar":"bookSidebar"},"physical-ai":{"id":"physical-ai","title":"Foundations of Physical AI","description":"Physical AI represents the cutting edge of artificial intelligence, where intelligent algorithms are embodied in physical agents—robots—that interact directly with the real world. Unlike purely digital AI, which operates within simulated or abstract environments, Physical AI confronts the complexities, uncertainties, and challenges of physics, material properties, and dynamic interactions inherent in our physical universe. This chapter lays the groundwork for understanding this exciting domain, exploring its core concepts and the unique demands it places on AI systems.","sidebar":"bookSidebar"},"references":{"id":"references","title":"References","description":"This section lists all academic and technical sources cited throughout the book, formatted according to APA 7th edition guidelines.","sidebar":"bookSidebar"},"ros2":{"id":"ros2","title":"ROS 2: The Robotic Nervous System","description":"The Robot Operating System (ROS) has become the de facto standard for robotics software development. ROS 2, its successor, offers significant improvements in areas critical for modern robotics, especially for humanoid platforms and Physical AI: real-time capabilities, security, and support for distributed systems. This chapter introduces the core concepts of ROS 2 and explains how it acts as the \"nervous system\" that orchestrates the complex interactions within a humanoid robot's software stack.","sidebar":"bookSidebar"},"vla":{"id":"vla","title":"Vision-Language-Action (VLA) Pipelines","description":"The ultimate goal of embodied AI is to create robots that can understand and respond to human intentions in a natural, intuitive way. This requires a seamless convergence of perception (vision), communication (language), and physical interaction (action). Vision-Language-Action (VLA) pipelines represent a cutting-edge approach to achieving this, leveraging recent advancements in large language models (LLMs) and multimodal AI to enable robots to interpret high-level commands, perceive complex environments, and execute sophisticated tasks.","sidebar":"bookSidebar"}}}}